{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Creacción SparkSession](#chapter1)\n",
    "* [Creacción DataFrame](#chapter2)\n",
    "* [Operaciones básicas](#chapter3)\n",
    "    * [Select](#sec3_1)\n",
    "    * [Filter](#sec3_2)\n",
    "    * [Aggregations](#sec3_3)\n",
    "    * [Sort](#sec3_4)\n",
    "* [DateTime](#chapter4)\n",
    "* [Funciones](#chapter5)\n",
    "* [Valores nulos](#chapter6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacción SparkSession <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession es el punto de entrada a PySpark, internamente crea SparkContext que es el motor interno que permite las conexiones con los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .appName('PySpark_Tutorial')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacción DataFrame <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "  \n",
    "pokedex = [(\"Bulbasaur\",\"Grass\",1,0.7),\n",
    "           (\"Ivysaur\",\"Grass\",2,1.0),\n",
    "           (\"Venusaur\",\"Grass\",3,2.0),\n",
    "           (\"Lugia\",\"Psychic\", 4, 5.2),\n",
    "           (\"Charmeleon\",\"Fire\",5,1.1),\n",
    "           (\"Charizard\",\"Fire\",6,1.7),\n",
    "           (\"Wartortle\",\"Water\",8,1.0),\n",
    "           (\"Blastoise\",\"Water\",9,1.6)]\n",
    " \n",
    "schema = [\"Name\",\"PrimaryType\",\"Index\",\"Height\"]\n",
    "df = spark.createDataFrame(data=pokedex, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+------+\n",
      "|      Name|PrimaryType|Index|Height|\n",
      "+----------+-----------+-----+------+\n",
      "| Bulbasaur|      Grass|    1|   0.7|\n",
      "|   Ivysaur|      Grass|    2|   1.0|\n",
      "|  Venusaur|      Grass|    3|   2.0|\n",
      "|     Lugia|    Psychic|    4|   5.2|\n",
      "|Charmeleon|       Fire|    5|   1.1|\n",
      "| Charizard|       Fire|    6|   1.7|\n",
      "| Wartortle|      Water|    8|   1.0|\n",
      "| Blastoise|      Water|    9|   1.6|\n",
      "+----------+-----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n",
      " Name        | Bulbasaur \n",
      " PrimaryType | Grass     \n",
      " id          | 1         \n",
      " Height      | 0.7       \n",
      "-RECORD 1----------------\n",
      " Name        | Ivysaur   \n",
      " PrimaryType | Grass     \n",
      " id          | 2         \n",
      " Height      | 1.0       \n",
      "-RECORD 2----------------\n",
      " Name        | Venusaur  \n",
      " PrimaryType | Grass     \n",
      " id          | 3         \n",
      " Height      | 2.0       \n",
      "-RECORD 3----------------\n",
      " Name        | Lugia     \n",
      " PrimaryType | Psychic   \n",
      " id          | 4         \n",
      " Height      | 5.2       \n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=4, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count**: cuenta el número de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**columns**: nombre de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'PrimaryType', 'Index', 'Height']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dtypes**: muestra el DateType de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('PrimaryType', 'string'),\n",
       " ('Index', 'bigint'),\n",
       " ('Height', 'double')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**printSchema**: muestra un esquema del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- PrimaryType: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumn**: toma dos parametros (nombre_columna, datos) y crea una nueva columna. `lit()` asigna una constante o valor literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+------+-------+\n",
      "|      Name|PrimaryType|Index|Height|   Game|\n",
      "+----------+-----------+-----+------+-------+\n",
      "| Bulbasaur|      Grass|    1|   0.7|Pokemon|\n",
      "|   Ivysaur|      Grass|    2|   1.0|Pokemon|\n",
      "|  Venusaur|      Grass|    3|   2.0|Pokemon|\n",
      "|     Lugia|    Psychic|    4|   5.2|Pokemon|\n",
      "|Charmeleon|       Fire|    5|   1.1|Pokemon|\n",
      "| Charizard|       Fire|    6|   1.7|Pokemon|\n",
      "| Wartortle|      Water|    8|   1.0|Pokemon|\n",
      "| Blastoise|      Water|    9|   1.6|Pokemon|\n",
      "+----------+-----------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Game\", lit(\"Pokemon\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden crear varias columnas a la vez y de dos métodos distintos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn(\"Game\",lit(\"Pokemon\")).withColumn(\"Generation\",lit(1))\n",
    "# df.select(col(\"Name\"),col(\"PrimaryType\"),col(\"Index\"),lit(\"Pokemon\").alias(\"Game\"),lit(1).alias(\"Generation\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumnRenamed**: toma dos parametros (nombre_antiguo, nombre_nuevo) y cambia el nombre de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'PrimaryType', 'id', 'Height']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumnRenamed('Index', 'id')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**drop**: elimina una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+------+\n",
      "|      Name|PrimaryType| id|Height|\n",
      "+----------+-----------+---+------+\n",
      "| Bulbasaur|      Grass|  1|   0.7|\n",
      "|   Ivysaur|      Grass|  2|   1.0|\n",
      "|  Venusaur|      Grass|  3|   2.0|\n",
      "|     Lugia|    Psychic|  4|   5.2|\n",
      "|Charmeleon|       Fire|  5|   1.1|\n",
      "| Charizard|       Fire|  6|   1.7|\n",
      "| Wartortle|      Water|  8|   1.0|\n",
      "| Blastoise|      Water|  9|   1.6|\n",
      "+----------+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('Game')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select <a class=\"anchor\" id=\"sec3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  1| Bulbasaur|\n",
      "|  2|   Ivysaur|\n",
      "|  3|  Venusaur|\n",
      "|  4|     Lugia|\n",
      "|  5|Charmeleon|\n",
      "|  6| Charizard|\n",
      "|  8| Wartortle|\n",
      "|  9| Blastoise|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter <a class=\"anchor\" id=\"sec3_2\"></a>\n",
    "Filtra las filas en base a determinada condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---+------+\n",
      "|     Name|PrimaryType| id|Height|\n",
      "+---------+-----------+---+------+\n",
      "|Bulbasaur|      Grass|  1|   0.7|\n",
      "+---------+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"id\"] == 1).show()\n",
    "\n",
    "    # Todas las siguientes ejecucciones tienen el mismo resultado\n",
    "#df.filter(df.id == 1).show()\n",
    "#df.filter(col(\"id\") == 1).show()\n",
    "#df.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vemos `between`, `when`, `rlike`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+------+\n",
      "|      Name|PrimaryType| id|Height|\n",
      "+----------+-----------+---+------+\n",
      "|   Ivysaur|      Grass|  2|   1.0|\n",
      "|  Venusaur|      Grass|  3|   2.0|\n",
      "|     Lugia|    Psychic|  4|   5.2|\n",
      "|Charmeleon|       Fire|  5|   1.1|\n",
      "+----------+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.id.between(2, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------------------------------------+\n",
      "|      Name|Height|CASE WHEN (Height >= 1.5) THEN 1 ELSE 0 END|\n",
      "+----------+------+-------------------------------------------+\n",
      "| Bulbasaur|   0.7|                                          0|\n",
      "|   Ivysaur|   1.0|                                          0|\n",
      "|  Venusaur|   2.0|                                          1|\n",
      "|     Lugia|   5.2|                                          1|\n",
      "|Charmeleon|   1.1|                                          0|\n",
      "| Charizard|   1.7|                                          1|\n",
      "| Wartortle|   1.0|                                          0|\n",
      "| Blastoise|   1.6|                                          1|\n",
      "+----------+------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name', 'Height', f.when(df.Height >= 1.5, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------+\n",
      "|      Name|PrimaryType|Tipo empieza por G|\n",
      "+----------+-----------+------------------+\n",
      "| Bulbasaur|      Grass|              true|\n",
      "|   Ivysaur|      Grass|              true|\n",
      "|  Venusaur|      Grass|              true|\n",
      "|     Lugia|    Psychic|             false|\n",
      "|Charmeleon|       Fire|             false|\n",
      "| Charizard|       Fire|             false|\n",
      "| Wartortle|      Water|             false|\n",
      "| Blastoise|      Water|             false|\n",
      "+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name','PrimaryType', df.PrimaryType.rlike('^G').alias('Tipo empieza por G')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations <a class=\"anchor\" id=\"sec3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------+\n",
      "|PrimaryType|Total|Total_height|\n",
      "+-----------+-----+------------+\n",
      "|      Water|    2|         2.6|\n",
      "|    Psychic|    1|         5.2|\n",
      "|       Fire|    2|         2.8|\n",
      "|      Grass|    3|         3.7|\n",
      "+-----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('PrimaryType').agg(\n",
    "    count(\"Name\").alias(\"Total\"),\n",
    "    sum(\"Height\").alias(\"Total_height\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort <a class=\"anchor\" id=\"sec3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+------+\n",
      "|      Name|PrimaryType| id|Height|\n",
      "+----------+-----------+---+------+\n",
      "| Bulbasaur|      Grass|  1|   0.7|\n",
      "| Wartortle|      Water|  8|   1.0|\n",
      "|   Ivysaur|      Grass|  2|   1.0|\n",
      "|Charmeleon|       Fire|  5|   1.1|\n",
      "| Blastoise|      Water|  9|   1.6|\n",
      "| Charizard|       Fire|  6|   1.7|\n",
      "|  Venusaur|      Grass|  3|   2.0|\n",
      "|     Lugia|    Psychic|  4|   5.2|\n",
      "+----------+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"Height\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateTime <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-------------------+\n",
      "| id|name| dept|salary|               date|\n",
      "+---+----+-----+------+-------------------+\n",
      "|  1| AAA|dept1|  1000|2019-02-01 15:12:13|\n",
      "|  2| BBB|dept1|  1100|  2018-04-01 5:12:3|\n",
      "|  3| CCC|dept1|  3000|  2017-06-05 1:2:13|\n",
      "|  4| DDD|dept1|  1500|2019-08-10 10:52:53|\n",
      "|  5| EEE|dept2|  8000| 2016-01-11 5:52:43|\n",
      "|  6| FFF|dept2|  7200|2015-04-14 19:32:33|\n",
      "|  7| GGG|dept3|  7100|2019-02-21 15:42:43|\n",
      "|  8| HHH|dept3|  3700|2016-09-25 15:32:33|\n",
      "|  9| III|dept3|  4500|2017-10-15 15:22:23|\n",
      "| 10| JJJ|dept5|  3400|2018-12-17 15:14:17|\n",
      "+---+----+-----+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000, \"2019-02-01 15:12:13\"),\n",
    "    (2, \"BBB\", \"dept1\", 1100, \"2018-04-01 5:12:3\"),\n",
    "    (3, \"CCC\", \"dept1\", 3000, \"2017-06-05 1:2:13\"),\n",
    "    (4, \"DDD\", \"dept1\", 1500, \"2019-08-10 10:52:53\"),\n",
    "    (5, \"EEE\", \"dept2\", 8000, \"2016-01-11 5:52:43\"),\n",
    "    (6, \"FFF\", \"dept2\", 7200, \"2015-04-14 19:32:33\"),\n",
    "    (7, \"GGG\", \"dept3\", 7100, \"2019-02-21 15:42:43\"),\n",
    "    (8, \"HHH\", \"dept3\", 3700, \"2016-09-25 15:32:33\"),\n",
    "    (9, \"III\", \"dept3\", 4500, \"2017-10-15 15:22:23\"),\n",
    "    (10, \"JJJ\", \"dept5\", 3400, \"2018-12-17 15:14:17\")]\n",
    "empdf = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\", \"date\"])\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_months`: añadimos una columna llamada <i>next_month</i> generada a partir de añadir un mes a <i>date</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|next_month|\n",
      "+-------------------+----------+\n",
      "|2019-02-01 15:12:13|2019-03-01|\n",
      "|  2018-04-01 5:12:3|2018-05-01|\n",
      "|  2017-06-05 1:2:13|2017-07-05|\n",
      "|2019-08-10 10:52:53|2019-09-10|\n",
      "| 2016-01-11 5:52:43|2016-02-11|\n",
      "|2015-04-14 19:32:33|2015-05-14|\n",
      "|2019-02-21 15:42:43|2019-03-21|\n",
      "|2016-09-25 15:32:33|2016-10-25|\n",
      "|2017-10-15 15:22:23|2017-11-15|\n",
      "|2018-12-17 15:14:17|2019-01-17|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"next_month\", add_months(\"date\", 1)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`date_add`: añadimos 5 días a la fecha leida. `date_sub` sería lo contrario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date| next_date|\n",
      "+-------------------+----------+\n",
      "|2019-02-01 15:12:13|2019-02-06|\n",
      "|  2018-04-01 5:12:3|2018-04-06|\n",
      "+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"next_date\", date_add(\"date\", 5)))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`current_date` y `current_timestamp`: fecha actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------------+\n",
      "|id |current_date|current_timestamp         |\n",
      "+---+------------+--------------------------+\n",
      "|1  |2021-04-05  |2021-04-05 08:04:13.670002|\n",
      "|2  |2021-04-05  |2021-04-05 08:04:13.670002|\n",
      "+---+------------+--------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .withColumn(\"current_date\", current_date())\n",
    "    .withColumn(\"current_timestamp\", current_timestamp())  \n",
    "    .select(\"id\", \"current_date\",\"current_timestamp\"))\n",
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`date_format`: convierte el formato de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|  new_date|\n",
      "+-------------------+----------+\n",
      "|2019-02-01 15:12:13|01/02/2019|\n",
      "|  2018-04-01 5:12:3|01/04/2018|\n",
      "+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"new_date\", date_format(\"date\", \"dd/MM/yyyy\")))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`date_trunc`: retorna timestamp truncado a una unidad de tiempo específica. Podemos cambiar month por: year, week, day, minute, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|               date|           new_date|\n",
      "+-------------------+-------------------+\n",
      "|2019-02-01 15:12:13|2019-02-01 00:00:00|\n",
      "|  2018-04-01 5:12:3|2018-04-01 00:00:00|\n",
      "|  2017-06-05 1:2:13|2017-06-01 00:00:00|\n",
      "|2019-08-10 10:52:53|2019-08-01 00:00:00|\n",
      "| 2016-01-11 5:52:43|2016-01-01 00:00:00|\n",
      "|2015-04-14 19:32:33|2015-04-01 00:00:00|\n",
      "|2019-02-21 15:42:43|2019-02-01 00:00:00|\n",
      "|2016-09-25 15:32:33|2016-09-01 00:00:00|\n",
      "|2017-10-15 15:22:23|2017-10-01 00:00:00|\n",
      "|2018-12-17 15:14:17|2018-12-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"new_date\", date_trunc(\"month\", \"date\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datediff`: calcula el tiempo transcurrido entre dos fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------+\n",
      "|               date|current_date|date_diff|\n",
      "+-------------------+------------+---------+\n",
      "|2019-02-01 15:12:13|  2021-04-05|      794|\n",
      "|  2018-04-01 5:12:3|  2021-04-05|     1100|\n",
      "|  2017-06-05 1:2:13|  2021-04-05|     1400|\n",
      "|2019-08-10 10:52:53|  2021-04-05|      604|\n",
      "+-------------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf.select(\"date\")\n",
    "        .withColumn(\"current_date\", current_date()) \n",
    "        .withColumn(\"date_diff\", datediff(\"current_date\", \"date\"))) \n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dayofmonth`, `dayofweek`, `dayofyear`: día del mes, semana o año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---------+---------+\n",
      "|               date|dayofmonth|dayofweek|dayofyear|\n",
      "+-------------------+----------+---------+---------+\n",
      "|2019-02-01 15:12:13|         1|        6|       32|\n",
      "|  2018-04-01 5:12:3|         1|        1|       91|\n",
      "|  2017-06-05 1:2:13|         5|        2|      156|\n",
      "+-------------------+----------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"dayofmonth\", dayofmonth(\"date\"))\n",
    "    .withColumn(\"dayofweek\", dayofweek(\"date\"))\n",
    "    .withColumn(\"dayofyear\", dayofyear(\"date\")))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hour`: devuelve la hora del formato fecha. También se puede consultar minute, month, year, weekofyear,  etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|               date|hour|\n",
      "+-------------------+----+\n",
      "|2019-02-01 15:12:13|  15|\n",
      "|  2018-04-01 5:12:3|   5|\n",
      "+-------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"hour\", hour(\"date\")))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_date`: convierte un string o timestamp en Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|   to_date|\n",
      "+-------------------+----------+\n",
      "|2019-02-01 15:12:13|2019-02-01|\n",
      "|  2018-04-01 5:12:3|2018-04-01|\n",
      "+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (empdf\n",
    "    .select(\"date\")\n",
    "    .withColumn(\"to_date\", to_date(\"date\")))\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el formato de la fecha es distinto lo especificamos, un ejemplo a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|  new_date|\n",
      "+-------------------+----------+\n",
      "|15/02/2019 10:30:00|2019-02-15|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([('15/02/2019 10:30:00',)], ['date'])\n",
    "df2 = (df1\n",
    "    .withColumn(\"new_date\", to_date(\"date\", 'dd/MM/yyyy HH:mm:ss')))    \n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (UDF) <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un DataFrame para aplicar UDF y más adelante analizar los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+\n",
      "|  id|name| dept|salary|\n",
      "+----+----+-----+------+\n",
      "|   1| AAA|dept1|  1000|\n",
      "|   2| BBB|dept1|  1100|\n",
      "|   3| CCC|dept1|  3000|\n",
      "|   4| DDD|dept1|  1500|\n",
      "|   5| EEE|dept2|  8000|\n",
      "|   6| FFF|dept2|  7200|\n",
      "|   7| GGG|dept3|  7100|\n",
      "|null|null| null|  7500|\n",
      "|   9| III| null|  4500|\n",
      "|  10|null|dept5|  2500|\n",
      "+----+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos agrupar el salario en tres niveles, para ello creamos una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detSalary_Level(sal):\n",
    "    level = None\n",
    "\n",
    "    if(sal > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(sal > 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(sal > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que registrar la función como UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la función sobre una columna determinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores nulos o NaN <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**isNull()** y **inNotNull()**: nos dan las filas con valores nulos o sin ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|  id|name|dept|salary|\n",
      "+----+----+----+------+\n",
      "|null|null|null|  7500|\n",
      "|   9| III|null|  4500|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNotNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fillna()**: sobreescribimos los valores nulos de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+------+\n",
      "|  id|name|   dept|salary|\n",
      "+----+----+-------+------+\n",
      "|   1| AAA|  dept1|  1000|\n",
      "|   2| BBB|  dept1|  1100|\n",
      "|   3| CCC|  dept1|  3000|\n",
      "|   4| DDD|  dept1|  1500|\n",
      "|   5| EEE|  dept2|  8000|\n",
      "|   6| FFF|  dept2|  7200|\n",
      "|   7| GGG|  dept3|  7100|\n",
      "|null|null|no_data|  7500|\n",
      "|   9| III|no_data|  4500|\n",
      "|  10|null|  dept5|  2500|\n",
      "+----+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.fillna(\"no_data\", [\"dept\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dropna()**: remueve las filas con valores nulosº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna() #df.dropna(how=\"any\") por defecto\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicarlo sobre una columna lo señalamos en <i>subset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna(subset = \"dept\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiar particiones: https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0#:~:text=Spark%20can%20run%201%20concurrent,2%E2%80%933x%20times%20more).\n",
    "\n",
    "Continuar tuto: https://github.com/NeerajBhadani/bigdata-ml/blob/master/apache-spark/getting-started-with-apache-spark-part-3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Webgraphy:\n",
    " - https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f\n",
    " - https://www.guru99.com/pyspark-tutorial.html\n",
    " - https://amiradata.com/pyspark-lit-function-to-add-literal-constant-column/\n",
    " - https://github.com/NeerajBhadani/bigdata-ml/blob/master/apache-spark/getting-started-with-apache-spark-part-2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
